{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的 ##\n",
    "\n",
    "はじめてのパターン認識の内容を自分の理解に合わせてまとめる。\n",
    "\n",
    "体系的に学ぶことを目指すため、ポイントごとに　`python` で実装する。\n",
    "また、ライブラリとして提供されているものがある場合、そのライブラリについてもキャッチアップする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 識別規則 \n",
    "→ いわゆる、判別方法のこと\n",
    "\n",
    "代表的なものに以下のものがある。\n",
    "\n",
    "- 事後確率による方法\n",
    "- 距離による方法\n",
    "- 関数値による方法\n",
    "- 決定木による方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師あり学習\n",
    "\n",
    "`概要について`\n",
    "\n",
    "識別規則として、入力データ（特徴ベクトル）からクラスへの写像を $y = f(\\vec{x})$ という関数を用いる\n",
    "学習データから　$f()$ を用いて決めることで、学習を行う\n",
    "\n",
    "２クラス問題の線形識別関数の場合、識別規則は\n",
    "\n",
    "$$ y = f(\\vec{x};\\vec{w}) = w_1x_1 + ・・・ + w_dx_d = \\vec{w}^T ・ \\vec{x} $$\n",
    "\n",
    "となる。このように、パラメータw と 入力ベクトルx の線形関数(内積) を用いて表現される。\n",
    "このパラメータwを調節することで、入力データが正しいクラスに対応する関数値を出すように学習することが目的である。\n",
    "\n",
    "\n",
    "`線形関数について`\n",
    "\n",
    "関数$f(x)$が線形である条件は、`重ね合わせの原理が成り立つ`ことが必要である。\n",
    "重ね合わせの原理とは、線形関数であるならば、$f(ax) = af(x), f(x+y) = f(x) + f(y)$が成り立つという性質を示したものである。\n",
    "一般に線形識別関数として使用される、$f(x) = w_1x + b$ のような、定数項がついた関数は厳密には線形とは言えない。このような関数をアフィン関数という。\n",
    "\n",
    "\n",
    "`学習について`\n",
    "\n",
    "学習を行うためには入力データとそのクラスを指定したデータをついにした学習データが必要となる。\n",
    "ここで、クラスを指定したデータを教師データと呼び、２クラスの場合は 0 or 1 の値を取りうる。\n",
    "このように学習に用いられる全ての対の集合を学習データセットと呼び $D_L$ で表す\n",
    "\n",
    "学習の目的は学習データを正しく識別できる $\\vec{w}$ を求めることである。\n",
    "方法としては、解析的に求める方法や、少しずつパラメータを更新して求める方法がある。\n",
    "\n",
    "\n",
    "`線形回帰について`\n",
    "教師データとして与えられている値が、2値ではなく任意の関数値が与えられている場合は、識別関数も入力に対して与えられた関数値を出力するように学習が行われる。\n",
    "そのために、識別関数 $f(x)$ は与えられた関数値を近似する能力を持つ必要がある。\n",
    "このような問題を関数近似（回帰）と呼ぶ。また、線形関数で近似する場合は線形回帰と呼ばれる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師なし学習\n",
    "\n",
    "`概要について`\n",
    "教師データがない学習も存在する。\n",
    "この場合は、入力データ間の距離や類似度、統計的な性質に基づいてクラスを自動的に生成する。これを `クラスタリング` と呼ぶ。\n",
    "\n",
    "このように教師データを用いない学習を 教師なし学習 あるいは 自己組織型学習と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 汎化能力について\n",
    "\n",
    "学習とは、学習データに対する識別関数の出力値と教師データとの誤差が最小になるように識別関数のパラメータを調整することである。\n",
    "しかし、学習の結果で得られた識別関数が学習データに含まれていない未知データに対してうまく働くという保証はない。\n",
    "そこで、学習データからテストデータを生成して、テストデータを用いて学習結果を評価して、予測を行うことが行われる。\n",
    "このようにすると、未知データに対してもうまく働くようになる。\n",
    "この未知データに対する識別能力を `汎化能力` と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テストデータについて\n",
    "\n",
    "学習データセット $D_L$ とテストデータセット $D_T$ は手元にあるデータを分割して作ることになる。\n",
    "\n",
    "ここで、対象とするデータ集合の全ての値の集合を ` 母集団` と呼び、その $d$ 次元の特徴の分布 $p$ で表現し、`真の分布` と呼ぶ。\n",
    "したがって、学習データやテストデータは真の分布からランダムにサンプリングされたものであり、各特徴の平均値や分散が真の分布と同じになるとは限らない。\n",
    "このズレを `偏り（バイアス）` と呼ぶ。\n",
    "\n",
    "ここで、 $D_L$ のデータを用いて学習を行い $D_T$ のデー タを用いてテストした時の誤り率を $\\epsilon (p_L,p_L)$ とすると、`真の誤り率` $\\epsilon(p,p)$ は真の分布 $p$ に従う学習データを用いて設計し、真の分布 $p$ に従うテストデータを用いてテストした時の誤り率を表す。\n",
    "\n",
    "学習データをテストデータとして用いて測定した誤り率を `再代入誤り率` と呼び、 $\\epsilon(p_L, p_L)$ で表す。\n",
    "\n",
    "####　生成の仕方\n",
    "\n",
    "手元にあるテストデータを学習用とテスト用に分割する代表的な方法は次のようなものがある。\n",
    "\n",
    "- ホールドアウト法\n",
    "- 交差確認法\n",
    "- 一つ抜き法\n",
    "- ブートストラップ法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
